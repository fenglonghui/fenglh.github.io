### 构建知识库分享总结

    使用LLamaIndex 调用本地大模型, 向大模型直接提问, 如果大模型在没接触过的知识领域, 同样也会回复,但回答的结果是一本正紧的胡说八道, 即产生大模型幻觉

    同样的逻辑: 大模型本地私有化部署,同样会产生大模型幻觉, 这个问题的解决方案就是构建知识库(即 构建RAG)

    RAG有一定的执行流程:

        用户向大模型提问, 大模型对用户问题语义进行理解并分析, 然后交给知识库, 知识库会进行向量检索查询, 大模型依据知识库检索查询结果进行匹配、总结,最后输出答复结果

    要实现这整个流程, 就需要有知识库, 即 RAG!!!

#### 1.构建知识库
       要构建知识库, 就需要用到LLamaIndex中的"数据连接器", 本项目是读取本地Markdown文件内容(说明:内容质量混乱), 所以就需要使用数据连接器SimpleDirectoryReader(并行处理数据)

       SimpleDirectoryReader 数据连接器: 解析读取本地文件数据
           
       该数据连接器支持文件类型:  markdown, csv, pdf txt

#### 词嵌入模型Embedding

     Embedding 模型大小和输入数据的维度大小有关
     唯一的作用功能: 将文本转为词向量, 跟微调的效果没有啥关系

     选择Embedding模型主要还得看物料的实际情况而定, 是纯中文, 还是纯英文 或者是中文和英文都存在
         1. 如果物料都是中文, 就找一个支持中文比较好的Embedding模型
         2. 如果物料都是英文, 就找一个支持英文比较好的Embedding模型
         3. 如果物料中中文、英文都有, 就找一个中英文都支持的Embedding模型

     本项目选择 sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 词嵌入模型, 这是个中文、英文都支持的嵌入模型

     每个大模型的结构中的第一部分都是Embedding部分, 大模型也可以作为词嵌入模型

#### 纯RAG知识库
        ```
          安装依赖环境:
             python 虚拟环境                         3.10
             llama-index-embeddings-huggingface     0.6.1
             llama-index-llms-huggingface           0.6.1
             llama-index                            0.14.4
             huggingface-hub                        0.36.0
             transformers                           4.56.2
    
             pip install llama-index-llms-huggingface==0.6.1
             pip install "transformers[torch]==4.56.2"
             pip install "huggingface_hub[inference]==0.36.0"
             pip install llama-index==0.14.4
        ```
     
     执行调用过程如下:
     ```
         from llama_index.embeddings.huggingface import HuggingFaceEmbedding
         from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex    # 数据连接器
         from llama_index.llms.huggingface import HuggingFaceLLM
        
        
         # 词嵌入模型
         embed_model = HuggingFaceEmbedding(
           # 指定了一个预训练sentence-transformer模型的路径
           model_name="/root/autodl-tmp/llms/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
         )
        
         Settings.embed_model = embed_model
         # 读取并加载文档
         documents = SimpleDirectoryReader("/root/autodl-tmp/project/data01").load_data()
         # 创建VectorStoreIndex, 并使用之前加载的文档来构建向量索引
         # 此索引将文档转换为向量, 并存储这些向量以便于快速检索
         index = VectorStoreIndex.from_documents(documents)  # 调用Embedding模型Settings.embed_model, 对文本进行转换, 将其转换成向量数据
         # 创建一个查询引擎, 这个引擎可接收以查询并返回相关文档的响应
         query_engine = index.as_query_engine()  
         # 这行代码操作会调用大模型Settings.llm对用户问题进行语义化理解并转换成词向量, 然后根据问题词向量和向量库的数据进行相似度检索查询并得到检索结果, 
         # 最后大模型根据检索结果进行匹配、总结输出回复
         rsp = query_engine.query("xtuner是什么?")  
         print(rsp)
     ```
       
     
     
